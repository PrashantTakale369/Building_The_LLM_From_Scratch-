# -*- coding: utf-8 -*-
"""Byte Pair Encoding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NHeVrSDoKzQdoPjhxfVrDb8IaHIpL7IP

This section covers a more sophisticated tokenization scheme based on a concept called byte pair encoding (BPE).
The BPE tokenizer covered in this section was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.
"""

# https://github.com/openai/tiktoken

# Tiktoken (OpenAI’s GPT-4 / GPT-4o / GPT-4.5)
# GPT-4 and GPT-4o use tiktoken, OpenAI’s efficient tokenizer.

# Based on byte-level BPE, but highly optimized for speed & memory.

# Backward-compatible with GPT-3 vocabulary but faster.

! pip3 install tiktoken

# 1 > Word Based Tokenizer.
# 2 > Sub-Word Based Tokenizer.
# 3 > Charecter Wised Tokenizer.

import importlib
import tiktoken

print("tiktoken version:", importlib.metadata.version("tiktoken"))

"""The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented previously via an encode method: https://github.com/PrashantTakale369/Transformer-Basics/blob/b0eb0d70b16f09b11f4c5e8bd99e803c8e51771e/Tokanizer/Tokanizer.ipynb"""

tokenizer = tiktoken.get_encoding("gpt2")

text = (
    "Hello, do you like tea? <|endoftext|> My name is Prashant someunknownPlace."
)

integers = tokenizer.encode(text, allowed_special={"<|endoftext|>"})

print(integers)

"""We can then convert the token IDs back into text using the decode method, similar to our SimpleTokenizerV2 earlier:
https://github.com/PrashantTakale369/Transformer-Basics/blob/b0eb0d70b16f09b11f4c5e8bd99e803c8e51771e/Tokanizer/Tokanizer.ipynb
"""

strings = tokenizer.decode(integers)
print(strings)

"""In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary size of 50,257, with <|endoftext|> being assigned the largest token ID.

Second, the BPE tokenizer above encodes and decodes unknown words, such as "someunknownPlace" correctly.
The BPE tokenizer can handle any unknown word. How does it achieve this without using <|unk|> tokens?

**Lets Try on Diff meningless word **
"""

text = (
    "jjnd difn"
)

integers = tokenizer.encode(text, allowed_special={"<|endoftext|>"})

print(integers)

strings = tokenizer.decode(integers)
print(strings)

