{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YanqRw-NGniz",
        "EUcKrjX6Z_Iq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Creating Tokens"
      ],
      "metadata": {
        "id": "YanqRw-NGniz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The print command prints the total number of characters followed by the first 100 characters of this file for illustration purposes."
      ],
      "metadata": {
        "id": "l6YbcqRcGwPj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bmp1wDkGjJe",
        "outputId": "7140647a-d444-4855-c01c-678945947d39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 71263\n",
            "TITLE: Alice's Adventures in Wonderland\n",
            "AUTHOR: Lewis Carroll\n",
            "\n",
            "= CHAPTER I = \n",
            "=( Down the Rabbit-Ho\n"
          ]
        }
      ],
      "source": [
        "with open(\"alice_in_wonderland.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using some simple example text, we can use the re.split command with the following syntax to split a text on whitespace characters:"
      ],
      "metadata": {
        "id": "J5pgu84AYQPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y_14FLWGpk-",
        "outputId": "088c06e4-bc90-493a-868c-f3f894a01121"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using some simple example text, we can use the re.split command with the following syntax to split a text on whitespace characters:"
      ],
      "metadata": {
        "id": "Jvj2uOvIYUb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdlp5RLiX1gy",
        "outputId": "963ae580-177b-4588-97e8-8c8a51f44353"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's modify the regular expression (re) splits on whitespaces (\\s) and commas, and periods ([,.]):\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zu30jnSdYbVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZOujhJDYZBr",
        "outputId": "15d52557-383b-4d71-c378-e3680773ce17"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small remaining issue is that the list still includes whitespace characters. Optionally, we can remove these redundant characters safely as follows:\n"
      ],
      "metadata": {
        "id": "XCcZ2uPWY0Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]      #  item.strip() :- Removing the WhiteSpaces\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "widynVySYhuL",
        "outputId": "34711336-724d-4882-c678-8073c96434b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Let's modify it a bit further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double-dashes we have seen earlier in the first 100 characters of Edith Wharton's short story, along with additional special characters:"
      ],
      "metadata": {
        "id": "Giid-i7IZFHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a-ozORQY19d",
        "outputId": "7ad021c5-6898-43ec-e8f5-1d0047e502ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip whitespace from each item and then filter out any empty strings.\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ieLCYplZGvh",
        "outputId": "c52ebd50-1287-4ff2-8030-f023d9aef919"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now that we got a basic tokenizer working,"
      ],
      "metadata": {
        "id": "TwTd9A5IZmAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrqIs7cQZLBY",
        "outputId": "a31b3309-e492-47bc-9df2-bb33072e5dca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['TITLE', ':', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'AUTHOR', ':', 'Lewis', 'Carroll', '=', 'CHAPTER', 'I', '=', '=', '(', 'Down', 'the', 'Rabbit-Hole', ')', '=', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsrwiyW7ZuhI",
        "outputId": "54422581-5c1f-4ccc-b6ce-61d93d789fd9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if we want to remove the Numerical values form the Dataset then we can also do it here then after that creating the token id"
      ],
      "metadata": {
        "id": "Bg6m9utVa0Cq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Creating Token IDs"
      ],
      "metadata": {
        "id": "EUcKrjX6Z_Iq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous section, we tokenized a short story and assigned it to a Python variable called preprocessed. Let's now create a list of all unique tokens and sort them alphabetically to determine the vocabulary size:"
      ],
      "metadata": {
        "id": "Txc1HYilaKLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HtGfwVNZ8FB",
        "outputId": "c37f1b17-a3a0-477d-ee7e-af05b4287133"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After determining that the vocabulary size is 3189 via the above code, we create the vocabulary and print its first 51 entries for illustration purposes:"
      ],
      "metadata": {
        "id": "yYDWCWIsaVXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}   # assinning the token id to the vocab"
      ],
      "metadata": {
        "id": "h59iOegkaQzg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZNxgaNaaey0",
        "outputId": "f0c06b9b-f6b7-490d-a630-29bda5a12e1f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "('*', 5)\n",
            "(',', 6)\n",
            "('--', 7)\n",
            "('.', 8)\n",
            "(':', 9)\n",
            "(';', 10)\n",
            "('=', 11)\n",
            "('?', 12)\n",
            "('A', 13)\n",
            "('ALICE', 14)\n",
            "('ARE', 15)\n",
            "('AUTHOR', 16)\n",
            "('Ada', 17)\n",
            "('Adventures', 18)\n",
            "('Advice', 19)\n",
            "('After', 20)\n",
            "('Ah', 21)\n",
            "('Alas', 22)\n",
            "('Alice', 23)\n",
            "('Allow', 24)\n",
            "('An', 25)\n",
            "('And', 26)\n",
            "('Ann', 27)\n",
            "('Antipathies', 28)\n",
            "('As', 29)\n",
            "('At', 30)\n",
            "('Atheling', 31)\n",
            "('Australia', 32)\n",
            "('BEE', 33)\n",
            "('BUSY', 34)\n",
            "('Be', 35)\n",
            "('Because', 36)\n",
            "('Besides', 37)\n",
            "('Bill', 38)\n",
            "('Brandy', 39)\n",
            "('But', 40)\n",
            "('By', 41)\n",
            "('C', 42)\n",
            "('CAN', 43)\n",
            "('CHAPTER', 44)\n",
            "('CHORUS', 45)\n",
            "('COULD', 46)\n",
            "('CURTSEYING', 47)\n",
            "('Canary', 48)\n",
            "('Canterbury', 49)\n",
            "('Carroll', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Later in this book, when we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs into text.\n",
        "\n",
        "For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens."
      ],
      "metadata": {
        "id": "WLF9L8T_bOge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary.\n",
        "### In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text."
      ],
      "metadata": {
        "id": "_kBPfZlxbToT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
        "\n",
        "# Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
        "\n",
        "# Step 3: Process input text into token IDs\n",
        "\n",
        "# Step 4: Convert token IDs back into text\n",
        "\n",
        "# Step 5: Replace spaces before the specified punctuation"
      ],
      "metadata": {
        "id": "aCBv7q8LapgQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "zKoTSdMybzDN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 try it out in practice:"
      ],
      "metadata": {
        "id": "9qyjY9eTcMef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"Alice's Adventures in Wonderland\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXgTZ7cRb2yh",
        "outputId": "5bd4d5d6-07fc-48bb-d074-24aa2dae062c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 23, 2, 1603, 18, 1117, 264]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above prints the following token IDs: Next, let's see if we can turn these token IDs back into text using the decode method:"
      ],
      "metadata": {
        "id": "qKIWspUTeULH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "02XftDj2cbhK",
        "outputId": "6a087b34-c86d-44fc-e271-f75bc5ffb7b2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" Alice\\' s Adventures in Wonderland'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea?\"   # Hello this word is not in the Txt file so this error is getting so we are ====> adding special context\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "MSVSK7CPeWKq",
        "outputId": "aeffa526-f3fe-466b-92ee-8baadb2cbc58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-2211574883.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea?\"\u001b[0m   \u001b[0;31m# Hello this word is not in the Txt file so this error is getting so we are ====> adding special context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-17-2423359500.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ]\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-17-2423359500.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ]\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADDING SPECIAL CONTEXT TOKENS"
      ],
      "metadata": {
        "id": "uUfvYQ4WehSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In the previous section, we implemented a simple tokenizer and applied it to a passage from the training set.\n",
        "\n",
        "# In this section, we will modify this tokenizer to handle unknown words.\n",
        "\n",
        "# In particular, we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2\n",
        "# to support two new tokens, <|unk|> and <|endoftext|>"
      ],
      "metadata": {
        "id": "m04c5EQoeZM2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now modify the vocabulary to include these two special tokens, and <|endoftext|>, by adding these to the list of all unique words that we created in the previous section:"
      ],
      "metadata": {
        "id": "WVqpGG9jes0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "zP9wBu_JetDT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJMDtreHezNx",
        "outputId": "eab10733-8735-4ce1-8358-e4c90cc7825c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2098"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjYIiNePe0lp",
        "outputId": "5bc671e6-c870-4185-ecce-1ba7805f5b29"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('yourself', 2093)\n",
            "('youth', 2094)\n",
            "('zigzag', 2095)\n",
            "('<|endoftext|>', 2096)\n",
            "('<|unk|>', 2097)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9fRRNrkLe3kj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Replace unknown words by <|unk|> tokens\n",
        "\n",
        "Step 2: Replace spaces before the specified punctuations"
      ],
      "metadata": {
        "id": "Qdm5PccBe9IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int           # <===#\n",
        "            else \"<|unk|>\" for item in preprocessed   # <===#\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "dCz9XaIQe9Ya"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a2sBS0ZfHJh",
        "outputId": "c7f74545-c8c0-407d-8f6d-b42335ae21da"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x85EpcArfKBc",
        "outputId": "c4668876-b3f6-4561-f9c7-a57a86879452"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2097,\n",
              " 6,\n",
              " 780,\n",
              " 2090,\n",
              " 1222,\n",
              " 2097,\n",
              " 12,\n",
              " 2096,\n",
              " 119,\n",
              " 1860,\n",
              " 2097,\n",
              " 2097,\n",
              " 1377,\n",
              " 1860,\n",
              " 2097,\n",
              " 8]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oDq7-vEcNmwc",
        "outputId": "fc19dad4-3d72-48d4-b03d-14b688ad90a2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like <|unk|>? <|endoftext|> In the <|unk|> <|unk|> of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE (Byte Pair Encoding)"
      ],
      "metadata": {
        "id": "ff3H-pzANppB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section covers a more sophisticated tokenization scheme based on a concept called byte pair encoding (BPE).\n",
        "The BPE tokenizer covered in this section was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT."
      ],
      "metadata": {
        "id": "8bTy8fcGNzRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/openai/tiktoken"
      ],
      "metadata": {
        "id": "ejjDtYVxNoPB"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tiktoken (OpenAI’s GPT-4 / GPT-4o / GPT-4.5)\n",
        "# GPT-4 and GPT-4o use tiktoken, OpenAI’s efficient tokenizer.\n",
        "\n",
        "# Based on byte-level BPE, but highly optimized for speed & memory.\n",
        "\n",
        "# Backward-compatible with GPT-3 vocabulary but faster.\n"
      ],
      "metadata": {
        "id": "Lpq-fcf5N5cq"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 > Word Based Tokenizer.\n",
        "# 2 > Sub-Word Based Tokenizer.\n",
        "# 3 > Charecter Wised Tokenizer."
      ],
      "metadata": {
        "id": "r5Idh7g8N77-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented previously via an encode method: https://github.com/PrashantTakale369/Transformer-Basics/blob/b0eb0d70b16f09b11f4c5e8bd99e803c8e51771e/Tokanizer/Tokanizer.ipynb"
      ],
      "metadata": {
        "id": "MJNB8VldOAN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugvj6sSiN9sG",
        "outputId": "4efc4a65-1925-49ed-8693-42a5448ebd62"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "W6rdxR1dOFqm"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> My name is Prashant someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE-4Lo1EOIWa",
        "outputId": "80d7cb20-f386-413c-8f5f-bf7346f9a9cd"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 2011, 1438, 318, 1736, 1077, 415, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then convert the token IDs back into text using the decode method, similar to our SimpleTokenizerV2 earlier: https://github.com/PrashantTakale369/Transformer-Basics/blob/b0eb0d70b16f09b11f4c5e8bd99e803c8e51771e/Tokanizer/Tokanizer.ipynb"
      ],
      "metadata": {
        "id": "hLI0v9gIOMTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL8iQXlwOIN0",
        "outputId": "12a77293-495e-49c1-b44a-328da07ba9d2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> My name is Prashant someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary size of 50,257, with <|endoftext|> being assigned the largest token ID.\n",
        "\n",
        "Second, the BPE tokenizer above encodes and decodes unknown words, such as \"someunknownPlace\" correctly. The BPE tokenizer can handle any unknown word. How does it achieve this without using <|unk|> tokens?\n",
        "\n",
        "*Lets Try on Diff meningless word *"
      ],
      "metadata": {
        "id": "FFP1hOeZOVJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"jjnd difn\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSkpzAIROXmq",
        "outputId": "383c25b1-1afb-45f6-e087-9f2c7bda1c7f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[41098, 358, 288, 361, 77]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGREI25YOaIe",
        "outputId": "ce37b5cc-73dd-4ccb-e634-f2748009e2d9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jjnd difn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader and Input Output Pairs"
      ],
      "metadata": {
        "id": "uvGQuxwnFv4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CREATING INPUT-TARGET PAIRS"
      ],
      "metadata": {
        "id": "axTqN5DTF_mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In this section we implement a data loader that fetches the input-target pairs using a sliding window approach.\n",
        "# To get started, we will first tokenize the whole The Verdict short story we worked with earlier using the BPE tokenizer introduced in the previous section:"
      ],
      "metadata": {
        "id": "r5YU-UBdfOSf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"alice_in_wonderland.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3105S3tGCz3",
        "outputId": "38661790-25da-4001-e2c8-e1e8b7da513c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "9l44wV9AGOxX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by 1:"
      ],
      "metadata": {
        "id": "mI-P7DZzHESY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4 #length of the input\n",
        "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens)\n",
        "#to predict the next word in the sequence.\n",
        "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NMhYeQ1Gz8u",
        "outputId": "89ed733f-d72a-4032-92e7-f0fed11f3b82"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [1024, 1434, 1131, 1860]\n",
            "y:      [1434, 1131, 1860, 557]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing the inputs along with the targets, which are the inputs shifted by one position, we can then create the next-word prediction tasks as follows:"
      ],
      "metadata": {
        "id": "gSVULjqcH9Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "id": "5PKa-MNLHI29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For understanding purposes, let's repeat the previous code but convert the token IDs into text:"
      ],
      "metadata": {
        "id": "KVGZfqzPIEYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSy45-ISIGzi",
        "outputId": "6bc96f9d-328f-460c-ead1-78e866d290a9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "had ----> peeped\n",
            "had peeped ----> into\n",
            "had peeped into ----> the\n",
            "had peeped into the ----> book\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to ===>\n",
        "implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays."
      ],
      "metadata": {
        "id": "WMcoFjZrIrD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTING A DATA LOADER"
      ],
      "metadata": {
        "id": "hvONionMIi0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Tokenize the entire text\n",
        "# Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "# Step 3: Return the total number of rows in the dataset\n",
        "# Step 4: Return a single row from the dataset"
      ],
      "metadata": {
        "id": "CED3BR8TIjoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "0inDdf26JVAO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row consists of a number of token IDs (based on a max_length) assigned to an input_chunk tensor.\n",
        "\n",
        "The target_chunk tensor contains the corresponding targets."
      ],
      "metadata": {
        "id": "OqMswhgpJVSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the tokenizer\n",
        "\n",
        "# Step 2: Create dataset\n",
        "\n",
        "# Step 3: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
        "\n",
        "# Step 4: The number of CPU processes to use for preprocessing"
      ],
      "metadata": {
        "id": "Hj7GxFv8JVxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnheuPV4KLFh",
        "outputId": "afb84fd7-0a8d-44e1-e92c-380464051281"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.7.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,stride=128, shuffle=True, drop_last=True,num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "K6BWL6CWJju9"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"alice_in_wonderland.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "OrmhUXxOJmSx"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next() function"
      ],
      "metadata": {
        "id": "KGNddstrJ2_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "-OwN94UDKaOm"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoMtJ0YkJw2k",
        "outputId": "c6ed6962-dedc-447b-cb27-c48c67511e02"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "[tensor([[49560,  2538,    25, 14862]]), tensor([[ 2538,    25, 14862,   338]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first_batch variable contains two tensors: the first tensor stores the input token IDs, and the second tensor stores the target token IDs.\n",
        "Since the max_length is set to 4, each of the two tensors contains 4 token IDs."
      ],
      "metadata": {
        "id": "g6WQQvIeKn3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In mostly LLM Has 256 maxLength :- means it predict the next word after 256 word yes"
      ],
      "metadata": {
        "id": "Tj2xPXnGJ_rE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " creating the embedding vectors from the token IDs,"
      ],
      "metadata": {
        "id": "Jq8W1usHLg4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DeBPBj5K6o3",
        "outputId": "fbe62759-a2d7-4bec-fdf4-7fb9f00903b0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[49560,  2538,    25, 14862],\n",
            "        [  338, 15640,   287, 42713],\n",
            "        [  198,    32, 24318,  1581],\n",
            "        [   25, 10174, 21298,   198],\n",
            "        [  198,    28,  5870, 29485],\n",
            "        [  314,   796,   220,   198],\n",
            "        [16193,  5588,   262, 25498],\n",
            "        [   12,    39,  2305,  1267]])\n",
            "\n",
            "Targets:\n",
            " tensor([[ 2538,    25, 14862,   338],\n",
            "        [15640,   287, 42713,   198],\n",
            "        [   32, 24318,  1581,    25],\n",
            "        [10174, 21298,   198,   198],\n",
            "        [   28,  5870, 29485,   314],\n",
            "        [  796,   220,   198, 16193],\n",
            "        [ 5588,   262, 25498,    12],\n",
            "        [   39,  2305,  1267,    28]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Jb86FdOLkDz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}