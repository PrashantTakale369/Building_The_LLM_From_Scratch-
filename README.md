# Building_The_LLM_From_Scratch-

## we are starting the Build an LLM ( Transform Model ) From the Scratche. 

Special thanks to *Sebastian Raschka*, author of Build a Large Language Model from Scratch, for providing a clear, hands-on guide to demystify the process of building LLMs step by step.
also the book is Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, Thomas Wolf (from Hugging Face) (Oâ€™Reilly, 2022)

# 1 Byte Pair Encoding
==> This section covers a more sophisticated tokenization scheme based on a concept called byte pair encoding (BPE). The BPE tokenizer covered in this section was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.
